{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689cbe5e-1f53-4739-a1c3-1410a2e0dd38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 01:06:49,043 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:06:49,047 - INFO - train_file num : 40\n",
      "2023-08-26 01:06:49,048 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:06:49,050 - INFO - train_file num : 40\n",
      "2023-08-26 01:06:49,051 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:06:49,056 - INFO - train_file num : 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|████████████████████████████████████████████████████████| 120/120 [00:04<00:00, 28.09it/s]\n",
      "2023-08-26 01:06:53,332 - INFO - save_pickle -> ./pickle_pretrain_v4/pretrain_('fan', 'valve', 'slider').pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "66/66 [==============================] - 2s 6ms/step - loss: 224.4867 - val_loss: 19.6667\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 17.0049 - val_loss: 18.8950\n",
      "Epoch 3/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 16.8392 - val_loss: 18.7872\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 16.7728 - val_loss: 18.6711\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 16.2229 - val_loss: 16.9806\n",
      "Epoch 6/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 14.1038 - val_loss: 14.8346\n",
      "Epoch 7/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.4969 - val_loss: 14.6882\n",
      "Epoch 8/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.4123 - val_loss: 14.6374\n",
      "Epoch 9/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.3423 - val_loss: 14.5279\n",
      "Epoch 10/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.2414 - val_loss: 14.3322\n",
      "Epoch 11/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 13.0445 - val_loss: 14.0321\n",
      "Epoch 12/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 12.7149 - val_loss: 13.5695\n",
      "Epoch 13/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 12.3637 - val_loss: 13.1145\n",
      "Epoch 14/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.1186 - val_loss: 12.8525\n",
      "Epoch 15/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9638 - val_loss: 12.5669\n",
      "Epoch 16/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.8310 - val_loss: 12.4494\n",
      "Epoch 17/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.7100 - val_loss: 12.2317\n",
      "Epoch 18/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.5746 - val_loss: 12.0297\n",
      "Epoch 19/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.4529 - val_loss: 11.8855\n",
      "Epoch 20/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.3396 - val_loss: 11.5755\n",
      "Epoch 21/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.2392 - val_loss: 11.4202\n",
      "Epoch 22/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.1617 - val_loss: 11.3603\n",
      "Epoch 23/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.0996 - val_loss: 11.1764\n",
      "Epoch 24/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 11.0133 - val_loss: 11.0864\n",
      "Epoch 25/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.9356 - val_loss: 11.0335\n",
      "Epoch 26/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.8292 - val_loss: 10.9475\n",
      "Epoch 27/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.6961 - val_loss: 10.8882\n",
      "Epoch 28/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5844 - val_loss: 10.8301\n",
      "Epoch 29/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 10.4771 - val_loss: 10.7877\n",
      "Epoch 30/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.3748 - val_loss: 10.7679\n",
      "Epoch 31/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.3152 - val_loss: 10.7671\n",
      "Epoch 32/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.2460 - val_loss: 10.6677\n",
      "Epoch 33/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.2188 - val_loss: 10.8252\n",
      "Epoch 34/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.1833 - val_loss: 10.7167\n",
      "Epoch 35/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.1370 - val_loss: 10.7732\n",
      "Epoch 36/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.1175 - val_loss: 10.7112\n",
      "Epoch 37/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.0994 - val_loss: 10.7170\n",
      "Epoch 38/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 10.0818 - val_loss: 10.6937\n",
      "Epoch 39/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.0745 - val_loss: 10.7205\n",
      "Epoch 40/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.0382 - val_loss: 10.7058\n",
      "Epoch 41/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.0257 - val_loss: 10.7705\n",
      "Epoch 42/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 10.0057 - val_loss: 10.8298\n",
      "Epoch 43/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.9956 - val_loss: 10.7935\n",
      "Epoch 44/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 9.9682 - val_loss: 10.8245\n",
      "Epoch 45/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 9.9475 - val_loss: 10.7510\n",
      "Epoch 46/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.9298 - val_loss: 10.8647\n",
      "Epoch 47/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.9221 - val_loss: 10.8486\n",
      "Epoch 48/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.8848 - val_loss: 10.8652\n",
      "Epoch 49/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.8624 - val_loss: 10.8429\n",
      "Epoch 50/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.8622 - val_loss: 10.8729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baejs\\anaconda3\\envs\\bae\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "2023-08-26 01:07:07,011 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:07,013 - INFO - train_file num : 40\n",
      "2023-08-26 01:07:07,014 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:07,016 - INFO - train_file num : 40\n",
      "2023-08-26 01:07:07,017 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:07,019 - INFO - train_file num : 40\n",
      "generate train_dataset: 100%|████████████████████████████████████████████████████████| 120/120 [00:01<00:00, 76.59it/s]\n",
      "2023-08-26 01:07:08,595 - INFO - save_pickle -> ./pickle_pretrain_v4/pretrain_('fan', 'valve', 'pump').pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "66/66 [==============================] - 1s 5ms/step - loss: 254.9041 - val_loss: 19.2730\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 16.7822 - val_loss: 18.2679\n",
      "Epoch 3/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 16.5605 - val_loss: 18.0352\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 16.4429 - val_loss: 17.7233\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 16.0432 - val_loss: 16.5033\n",
      "Epoch 6/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 15.1327 - val_loss: 14.3540\n",
      "Epoch 7/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 14.2419 - val_loss: 13.1656\n",
      "Epoch 8/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.7654 - val_loss: 12.6941\n",
      "Epoch 9/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.5168 - val_loss: 12.4510\n",
      "Epoch 10/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.3029 - val_loss: 12.2930\n",
      "Epoch 11/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.0621 - val_loss: 12.0380\n",
      "Epoch 12/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.7701 - val_loss: 11.9076\n",
      "Epoch 13/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.5311 - val_loss: 11.6019\n",
      "Epoch 14/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.3004 - val_loss: 11.4220\n",
      "Epoch 15/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.1197 - val_loss: 11.3598\n",
      "Epoch 16/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.7476 - val_loss: 11.1799\n",
      "Epoch 17/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.4433 - val_loss: 10.8705\n",
      "Epoch 18/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.2446 - val_loss: 10.5574\n",
      "Epoch 19/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.1937 - val_loss: 10.5331\n",
      "Epoch 20/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.1477 - val_loss: 10.4112\n",
      "Epoch 21/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.1009 - val_loss: 10.5283\n",
      "Epoch 22/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.0517 - val_loss: 10.4421\n",
      "Epoch 23/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.9861 - val_loss: 10.4628\n",
      "Epoch 24/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.9245 - val_loss: 10.4406\n",
      "Epoch 25/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.8610 - val_loss: 10.3972\n",
      "Epoch 26/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.8082 - val_loss: 10.2320\n",
      "Epoch 27/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.7437 - val_loss: 10.2705\n",
      "Epoch 28/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.7067 - val_loss: 10.3171\n",
      "Epoch 29/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.6638 - val_loss: 10.2422\n",
      "Epoch 30/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.6363 - val_loss: 10.1202\n",
      "Epoch 31/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.6195 - val_loss: 10.2482\n",
      "Epoch 32/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5969 - val_loss: 10.0550\n",
      "Epoch 33/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5925 - val_loss: 10.2109\n",
      "Epoch 34/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5775 - val_loss: 10.1441\n",
      "Epoch 35/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5615 - val_loss: 10.1555\n",
      "Epoch 36/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5577 - val_loss: 10.2398\n",
      "Epoch 37/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5594 - val_loss: 10.1021\n",
      "Epoch 38/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5478 - val_loss: 10.3001\n",
      "Epoch 39/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5430 - val_loss: 10.0163\n",
      "Epoch 40/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5434 - val_loss: 10.1602\n",
      "Epoch 41/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5531 - val_loss: 10.1815\n",
      "Epoch 42/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5436 - val_loss: 10.3263\n",
      "Epoch 43/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5284 - val_loss: 10.2664\n",
      "Epoch 44/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5284 - val_loss: 10.0740\n",
      "Epoch 45/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5373 - val_loss: 10.2214\n",
      "Epoch 46/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5300 - val_loss: 10.0140\n",
      "Epoch 47/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5206 - val_loss: 10.1725\n",
      "Epoch 48/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5381 - val_loss: 10.1441\n",
      "Epoch 49/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5195 - val_loss: 10.1059\n",
      "Epoch 50/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5166 - val_loss: 10.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 01:07:20,263 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:20,265 - INFO - train_file num : 40\n",
      "2023-08-26 01:07:20,266 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:20,268 - INFO - train_file num : 40\n",
      "2023-08-26 01:07:20,269 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:20,270 - INFO - train_file num : 40\n",
      "generate train_dataset: 100%|████████████████████████████████████████████████████████| 120/120 [00:01<00:00, 84.20it/s]\n",
      "2023-08-26 01:07:21,706 - INFO - save_pickle -> ./pickle_pretrain_v4/pretrain_('fan', 'slider', 'pump').pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "66/66 [==============================] - 1s 5ms/step - loss: 255.5876 - val_loss: 18.8559\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 15.7899 - val_loss: 18.0289\n",
      "Epoch 3/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 15.5645 - val_loss: 18.0125\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 15.4298 - val_loss: 17.6717\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 15.1701 - val_loss: 17.1706\n",
      "Epoch 6/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 14.6414 - val_loss: 16.1067\n",
      "Epoch 7/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 13.7319 - val_loss: 13.7487\n",
      "Epoch 8/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.7552 - val_loss: 12.8577\n",
      "Epoch 9/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.6111 - val_loss: 12.7659\n",
      "Epoch 10/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 12.5580 - val_loss: 12.6705\n",
      "Epoch 11/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.4984 - val_loss: 12.7076\n",
      "Epoch 12/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.4054 - val_loss: 12.4725\n",
      "Epoch 13/50\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 12.1147 - val_loss: 12.1311\n",
      "Epoch 14/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.5241 - val_loss: 11.8629\n",
      "Epoch 15/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.2049 - val_loss: 11.6526\n",
      "Epoch 16/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.0083 - val_loss: 11.4354\n",
      "Epoch 17/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.8232 - val_loss: 11.3730\n",
      "Epoch 18/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.6719 - val_loss: 11.3736\n",
      "Epoch 19/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.5451 - val_loss: 11.2530\n",
      "Epoch 20/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.4602 - val_loss: 11.2898\n",
      "Epoch 21/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.3880 - val_loss: 11.2072\n",
      "Epoch 22/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.3422 - val_loss: 11.0862\n",
      "Epoch 23/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.3024 - val_loss: 11.0226\n",
      "Epoch 24/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.2763 - val_loss: 10.9989\n",
      "Epoch 25/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.2381 - val_loss: 11.1015\n",
      "Epoch 26/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.2150 - val_loss: 10.8464\n",
      "Epoch 27/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.1846 - val_loss: 10.9164\n",
      "Epoch 28/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.1402 - val_loss: 10.7818\n",
      "Epoch 29/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.1128 - val_loss: 10.8113\n",
      "Epoch 30/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.0645 - val_loss: 10.6567\n",
      "Epoch 31/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 10.0168 - val_loss: 10.6385\n",
      "Epoch 32/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.9609 - val_loss: 10.5777\n",
      "Epoch 33/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.9242 - val_loss: 10.4243\n",
      "Epoch 34/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.8752 - val_loss: 10.3795\n",
      "Epoch 35/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.8162 - val_loss: 10.4540\n",
      "Epoch 36/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.7796 - val_loss: 10.4135\n",
      "Epoch 37/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.7470 - val_loss: 10.2334\n",
      "Epoch 38/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.7059 - val_loss: 10.0773\n",
      "Epoch 39/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.6588 - val_loss: 10.2742\n",
      "Epoch 40/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.6224 - val_loss: 10.1591\n",
      "Epoch 41/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.6098 - val_loss: 10.3510\n",
      "Epoch 42/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.5825 - val_loss: 9.8602\n",
      "Epoch 43/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.5427 - val_loss: 10.3766\n",
      "Epoch 44/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.4948 - val_loss: 10.0868\n",
      "Epoch 45/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.4935 - val_loss: 9.9799\n",
      "Epoch 46/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.4580 - val_loss: 10.1534\n",
      "Epoch 47/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.4533 - val_loss: 10.1541\n",
      "Epoch 48/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.4227 - val_loss: 10.2234\n",
      "Epoch 49/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.4025 - val_loss: 9.8768\n",
      "Epoch 50/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 9.3890 - val_loss: 10.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 01:07:33,511 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:33,513 - INFO - train_file num : 40\n",
      "2023-08-26 01:07:33,513 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:33,515 - INFO - train_file num : 40\n",
      "2023-08-26 01:07:33,516 - INFO - target_dir : ../Sample_data\n",
      "2023-08-26 01:07:33,517 - INFO - train_file num : 40\n",
      "generate train_dataset: 100%|████████████████████████████████████████████████████████| 120/120 [00:01<00:00, 83.85it/s]\n",
      "2023-08-26 01:07:34,958 - INFO - save_pickle -> ./pickle_pretrain_v4/pretrain_('valve', 'slider', 'pump').pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "66/66 [==============================] - 1s 5ms/step - loss: 218.0794 - val_loss: 20.0901\n",
      "Epoch 2/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 18.1206 - val_loss: 19.4510\n",
      "Epoch 3/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 17.9394 - val_loss: 19.3102\n",
      "Epoch 4/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 17.8647 - val_loss: 19.3013\n",
      "Epoch 5/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 17.7363 - val_loss: 18.8085\n",
      "Epoch 6/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 17.3186 - val_loss: 17.3318\n",
      "Epoch 7/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 16.1687 - val_loss: 14.4547\n",
      "Epoch 8/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 15.1014 - val_loss: 13.2595\n",
      "Epoch 9/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 14.6135 - val_loss: 12.8804\n",
      "Epoch 10/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 14.1635 - val_loss: 12.4544\n",
      "Epoch 11/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 13.5370 - val_loss: 12.0816\n",
      "Epoch 12/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.9441 - val_loss: 11.7776\n",
      "Epoch 13/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.5500 - val_loss: 11.5572\n",
      "Epoch 14/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.3159 - val_loss: 11.3650\n",
      "Epoch 15/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.1811 - val_loss: 11.2131\n",
      "Epoch 16/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.1024 - val_loss: 11.2031\n",
      "Epoch 17/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.0614 - val_loss: 11.1787\n",
      "Epoch 18/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.0250 - val_loss: 11.2367\n",
      "Epoch 19/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 12.0108 - val_loss: 11.1647\n",
      "Epoch 20/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9825 - val_loss: 11.1243\n",
      "Epoch 21/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9746 - val_loss: 11.0039\n",
      "Epoch 22/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9673 - val_loss: 10.9236\n",
      "Epoch 23/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9484 - val_loss: 10.8973\n",
      "Epoch 24/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9307 - val_loss: 10.8874\n",
      "Epoch 25/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9120 - val_loss: 10.9763\n",
      "Epoch 26/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.9052 - val_loss: 10.7217\n",
      "Epoch 27/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.8862 - val_loss: 10.8797\n",
      "Epoch 28/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.8512 - val_loss: 10.7972\n",
      "Epoch 29/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.8271 - val_loss: 10.7396\n",
      "Epoch 30/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.7905 - val_loss: 10.6220\n",
      "Epoch 31/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.7613 - val_loss: 10.7273\n",
      "Epoch 32/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.7394 - val_loss: 10.5940\n",
      "Epoch 33/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.6961 - val_loss: 10.5417\n",
      "Epoch 34/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.6677 - val_loss: 10.6028\n",
      "Epoch 35/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.6354 - val_loss: 10.5030\n",
      "Epoch 36/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.6057 - val_loss: 10.3662\n",
      "Epoch 37/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.5764 - val_loss: 10.3922\n",
      "Epoch 38/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.5674 - val_loss: 10.3508\n",
      "Epoch 39/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.5291 - val_loss: 10.4590\n",
      "Epoch 40/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.5132 - val_loss: 10.2725\n",
      "Epoch 41/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.4957 - val_loss: 10.3659\n",
      "Epoch 42/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.4845 - val_loss: 10.4991\n",
      "Epoch 43/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.4798 - val_loss: 10.5478\n",
      "Epoch 44/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.4457 - val_loss: 10.4008\n",
      "Epoch 45/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.4384 - val_loss: 10.4934\n",
      "Epoch 46/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.4446 - val_loss: 10.2518\n",
      "Epoch 47/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.3955 - val_loss: 10.2595\n",
      "Epoch 48/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.3995 - val_loss: 10.4022\n",
      "Epoch 49/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.3918 - val_loss: 10.4450\n",
      "Epoch 50/50\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 11.3540 - val_loss: 10.4406\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# import default python-library\n",
    "########################################################################\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.core\n",
    "import librosa.feature\n",
    "import yaml\n",
    "import logging\n",
    "import keras\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "from itertools import combinations\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# setup STD I/O\n",
    "########################################################################\n",
    "# 로깅을 설정하고 초기화하는 부분\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"make_pretrain_v4.log\")\n",
    "logger = logging.getLogger(' ')\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# file I/O\n",
    "########################################################################\n",
    "#파일 입출력 관련 함수 선언 부분\n",
    "def save_pickle(filename, save_data):  \n",
    "    logger.info(\"save_pickle -> {}\".format(filename))\n",
    "    with open(filename, 'wb') as sf:\n",
    "        pickle.dump(save_data, sf)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    logger.info(\"load_pickle <- {}\".format(filename))\n",
    "    with open(filename, 'rb') as lf:\n",
    "        load_data = pickle.load(lf)\n",
    "    return load_data\n",
    "\n",
    "\n",
    "def file_load(wav_name, mono=False):\n",
    "    try:\n",
    "        return librosa.load(wav_name, sr=None, mono=mono)\n",
    "    except:\n",
    "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
    "\n",
    "\n",
    "def demux_wav(wav_name, channel=0):\n",
    "    try:\n",
    "        multi_channel_data, sr = file_load(wav_name)\n",
    "        if multi_channel_data.ndim <= 1:\n",
    "            return sr, multi_channel_data\n",
    "        return sr, np.array(multi_channel_data)[channel, :]\n",
    "    except ValueError as msg:\n",
    "        logger.warning(f'{msg}')\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# feature extractor\n",
    "########################################################################\n",
    "#소리 파일 하나를 로그멜스펙트로그램 형태로 바꾸고 딥러닝 모델에 넣을 형태로 바꾸는 함수 \n",
    "def file_to_vector_array(file_name, n_mels=64, frames=5, n_fft=1024, hop_length=512, power=2.0):\n",
    "    dims = n_mels * frames\n",
    "    sr, y = demux_wav(file_name)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
    "    log_mel_spectrogram = 20.0 / power * np.log10(mel_spectrogram + sys.float_info.epsilon)\n",
    "    vectorarray_size = len(log_mel_spectrogram[0, :]) - frames + 1\n",
    "    if vectorarray_size < 1:\n",
    "        return np.empty((0, dims), float)\n",
    "    vectorarray = np.zeros((vectorarray_size, dims), float)\n",
    "    for t in range(frames):\n",
    "        vectorarray[:, n_mels * t: n_mels * (t + 1)] = log_mel_spectrogram[:, t: t + vectorarray_size].T\n",
    "    return vectorarray\n",
    "\n",
    "\n",
    "#소리 파일들의 이름명이 담긴 리스트를 입력하면 그것들을 하나의 데이터셋으로 합치는 함수\n",
    "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, frames=5, n_fft=1024, hop_length=512, power=2.0):\n",
    "    dims = n_mels * frames\n",
    "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
    "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, frames=frames, n_fft=n_fft, hop_length=hop_length, power=power)\n",
    "        if idx == 0:\n",
    "            dataset = np.zeros((vector_array.shape[0] * len(file_list), dims), float)\n",
    "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
    "    return dataset\n",
    "\n",
    "def dataset_generator(target_dir, machine_type, normal_dir_name=\"normal\", ext=\"wav\"):\n",
    "    logger.info(\"target_dir : {}\".format(target_dir))\n",
    "    train_files = []\n",
    "    machine_id = f\"-6dB_{machine_type}\"\n",
    "    machine = f\"{machine_type}\"\n",
    "    id_list = [\"id_00\", \"id_02\", \"id_04\", \"id_06\"]\n",
    "    machine_type_path = os.path.join(target_dir, machine_id, machine)\n",
    "    for id_ in id_list:\n",
    "        machine_id = f\"{id_}\"\n",
    "        machine_id_path = os.path.join(machine_type_path, machine_id, normal_dir_name)\n",
    "        normal_files = sorted(glob.glob(os.path.join(machine_id_path, f\"*.{ext}\")))\n",
    "        train_files.extend(normal_files)\n",
    "    logger.info(\"train_file num : {num}\".format(num=len(train_files)))\n",
    "    return train_files\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# keras model\n",
    "########################################################################\n",
    "\n",
    "def keras_model(inputDim):\n",
    "    inputLayer = Input(shape=(inputDim,))\n",
    "    h = Dense(64, activation=\"relu\")(inputLayer)    \n",
    "    h = Dense(64, activation=\"relu\")(h)\n",
    "    h = Dense(8, activation=\"relu\")(h)\n",
    "    h = Dense(64, activation=\"relu\")(h)\n",
    "    h = Dense(64, activation=\"relu\")(h)\n",
    "    h = Dense(inputDim, activation=None)(h)\n",
    "    return Model(inputs=inputLayer, outputs=h)\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# main\n",
    "########################################################################\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"make_pretrain_v4.yaml\", encoding='utf-8') as stream:\n",
    "        pretrain_v4_param = yaml.safe_load(stream)\n",
    "    # pickle데이터, model데이터, result데이터가 저장될 파일과 폴더들 관련 변수 선언\n",
    "    os.makedirs(pretrain_v4_param[\"pickle_directory\"], exist_ok=True)\n",
    "    os.makedirs(pretrain_v4_param[\"model_directory\"], exist_ok=True)\n",
    "    pretrain_v4_data_dir = pretrain_v4_param[\"base_directory\"]\n",
    "    machine_types = [\"fan\", \"valve\", \"slider\", \"pump\"]\n",
    "    print(\"\\n===========================\")\n",
    "\n",
    "    # Combine machine types in pairs\n",
    "    for combo in combinations(machine_types, 3):\n",
    "        combo_ = f\"{combo}\"\n",
    "        \n",
    "        train_pickle = \"{pickle}/pretrain_{combo}.pickle\".format(pickle=pretrain_v4_param[\"pickle_directory\"],combo=combo_)\n",
    "        combo_train_files = []\n",
    "        for machine_type in combo:           \n",
    "            db = f\"-6dB_{machine_type}\"\n",
    "            train_files = dataset_generator(pretrain_v4_data_dir, machine_type)\n",
    "            combo_train_files.extend(train_files)\n",
    "        if os.path.exists(train_pickle):\n",
    "            train_data = load_pickle(train_pickle)\n",
    "        else:    \n",
    "            train_data = list_to_vector_array(combo_train_files,\n",
    "                                              msg=\"generate train_dataset\",\n",
    "                                              n_mels=pretrain_v4_param[\"feature\"][\"n_mels\"],\n",
    "                                              frames=pretrain_v4_param[\"feature\"][\"frames\"],\n",
    "                                              n_fft=pretrain_v4_param[\"feature\"][\"n_fft\"],\n",
    "                                              hop_length=pretrain_v4_param[\"feature\"][\"hop_length\"],\n",
    "                                              power=pretrain_v4_param[\"feature\"][\"power\"])\n",
    "            save_pickle(train_pickle, train_data)\n",
    "\n",
    "        print(\"============== MODEL TRAINING ==============\")\n",
    "        model_directory = pretrain_v4_param[\"model_directory\"]\n",
    "        model_file = \"{model}/pretrain_{combo}.h5\".format(model=model_directory, combo=combo)\n",
    "        \n",
    "        #모델 생성, 학습, 및 저장\n",
    "        if not os.path.exists(model_file):\n",
    "            model = keras_model(pretrain_v4_param[\"feature\"][\"n_mels\"] * pretrain_v4_param[\"feature\"][\"frames\"])\n",
    "            model.summary()\n",
    "            model.compile(**pretrain_v4_param[\"fit\"][\"compile\"])\n",
    "            model.fit(train_data, train_data, epochs=pretrain_v4_param[\"fit\"][\"epochs\"], batch_size=pretrain_v4_param[\"fit\"][\"batch_size\"], shuffle=pretrain_v4_param[\"fit\"][\"shuffle\"], validation_split=pretrain_v4_param[\"fit\"][\"validation_split\"], verbose=pretrain_v4_param[\"fit\"][\"verbose\"])\n",
    "            model.save(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92366e25-d3d5-438d-92bb-1c6110d8e6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
