{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53395b63-3d3c-43c4-a5be-63d889844ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 23:57:56,372 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:57:56,372 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:57:56,378 - INFO - train_file num : 40\n",
      "2023-08-25 23:57:56,378 - INFO - train_file num : 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate train_dataset: 100%|██████████████████████████████████████████████████████████| 40/40 [00:09<00:00,  4.05it/s]\n",
      "2023-08-25 23:58:06,282 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_fan.pickle\n",
      "2023-08-25 23:58:06,282 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_fan.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 650.8230 - val_loss: 228.9114\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 80.2305 - val_loss: 25.2440\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.9741 - val_loss: 13.4381\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.8660 - val_loss: 12.6112\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4544 - val_loss: 12.4182\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3768 - val_loss: 12.4688\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3449 - val_loss: 12.2136\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3230 - val_loss: 12.2077\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3066 - val_loss: 12.1941\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.2953 - val_loss: 12.1743\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2881 - val_loss: 12.1564\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2819 - val_loss: 12.1684\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.2789 - val_loss: 12.0982\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2718 - val_loss: 12.1805\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2658 - val_loss: 12.0787\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.2640 - val_loss: 12.2321\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2590 - val_loss: 12.1849\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2516 - val_loss: 12.2683\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.2472 - val_loss: 12.2332\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.2411 - val_loss: 12.0655\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2355 - val_loss: 12.1991\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2270 - val_loss: 12.1533\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.2139 - val_loss: 12.1127\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.1976 - val_loss: 11.8868\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.1788 - val_loss: 12.1726\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.1557 - val_loss: 12.1145\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.1200 - val_loss: 11.9279\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.0771 - val_loss: 11.8699\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.0114 - val_loss: 11.8051\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.9234 - val_loss: 11.9189\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.7931 - val_loss: 11.5039\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.6119 - val_loss: 11.3753\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.3581 - val_loss: 11.2360\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.0389 - val_loss: 10.6596\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 10.7036 - val_loss: 10.5267\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.4140 - val_loss: 10.3047\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.1903 - val_loss: 10.1525\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.0544 - val_loss: 10.0087\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.9596 - val_loss: 9.9854\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.9055 - val_loss: 9.8533\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.8762 - val_loss: 9.9570\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.8298 - val_loss: 9.9553\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.7914 - val_loss: 9.7193\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.7362 - val_loss: 9.7605\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.6664 - val_loss: 9.8853\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.5843 - val_loss: 9.7730\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.4969 - val_loss: 9.6684\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.4247 - val_loss: 9.8130\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.3663 - val_loss: 9.8108\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.3217 - val_loss: 9.7991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baejs\\anaconda3\\envs\\bae\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "2023-08-25 23:58:12,726 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:58:12,726 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:58:12,729 - INFO - train_file num : 40\n",
      "2023-08-25 23:58:12,729 - INFO - train_file num : 40\n",
      "generate train_dataset: 100%|██████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 68.21it/s]\n",
      "2023-08-25 23:58:13,327 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_valve.pickle\n",
      "2023-08-25 23:58:13,327 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_valve.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 10ms/step - loss: 604.9556 - val_loss: 188.1493\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 74.4344 - val_loss: 29.3624\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0014 - val_loss: 21.6993\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.0451 - val_loss: 20.8044\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.6517 - val_loss: 20.7670\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 19.5594 - val_loss: 20.6933\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.5172 - val_loss: 20.6582\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.4906 - val_loss: 20.6482\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.4660 - val_loss: 20.6598\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.4344 - val_loss: 20.5789\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.4026 - val_loss: 20.5551\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.3646 - val_loss: 20.5308\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.3069 - val_loss: 20.5150\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.2222 - val_loss: 20.5164\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.0865 - val_loss: 20.3636\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.8759 - val_loss: 20.2250\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.5423 - val_loss: 20.1222\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.0502 - val_loss: 20.0256\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4928 - val_loss: 19.8979\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 16.9885 - val_loss: 19.5315\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.6136 - val_loss: 19.6737\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.3807 - val_loss: 19.3876\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 16.2446 - val_loss: 19.4658\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.1625 - val_loss: 19.3177\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.1131 - val_loss: 19.3337\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 16.0740 - val_loss: 19.2043\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.0491 - val_loss: 19.2431\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 16.0267 - val_loss: 19.1699\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.0084 - val_loss: 19.0794\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9986 - val_loss: 19.1080\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9822 - val_loss: 19.1695\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9657 - val_loss: 18.8568\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9497 - val_loss: 19.0243\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9255 - val_loss: 18.8967\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 15.9011 - val_loss: 18.8038\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 15.8706 - val_loss: 18.6711\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 15.8096 - val_loss: 18.4603\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.7080 - val_loss: 18.2998\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.5375 - val_loss: 17.7850\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.3056 - val_loss: 17.2562\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.0282 - val_loss: 16.7430\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 14.7423 - val_loss: 16.1743\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.4855 - val_loss: 16.0586\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.3009 - val_loss: 16.0545\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.1876 - val_loss: 15.9484\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.1273 - val_loss: 15.7907\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 14.0776 - val_loss: 16.0732\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 14.0609 - val_loss: 15.8893\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.0380 - val_loss: 15.9375\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.0282 - val_loss: 15.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 23:58:19,768 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:58:19,768 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:58:19,771 - INFO - train_file num : 40\n",
      "2023-08-25 23:58:19,771 - INFO - train_file num : 40\n",
      "generate train_dataset: 100%|██████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 77.73it/s]\n",
      "2023-08-25 23:58:20,293 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_slider.pickle\n",
      "2023-08-25 23:58:20,293 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_slider.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 9ms/step - loss: 667.3950 - val_loss: 190.3683\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 74.3386 - val_loss: 29.3635\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.2933 - val_loss: 21.2473\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.1828 - val_loss: 20.8608\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.8332 - val_loss: 20.6981\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7706 - val_loss: 20.6637\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7469 - val_loss: 20.6341\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7310 - val_loss: 20.5785\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7214 - val_loss: 20.5079\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7174 - val_loss: 20.5479\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7075 - val_loss: 20.4470\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7032 - val_loss: 20.5494\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.6958 - val_loss: 20.5780\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 16.6891 - val_loss: 20.6591\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.6871 - val_loss: 20.6203\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 16.6766 - val_loss: 20.4419\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 16.6715 - val_loss: 20.6151\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.6598 - val_loss: 20.4335\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.6484 - val_loss: 20.4493\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.6353 - val_loss: 20.5122\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.6135 - val_loss: 20.4814\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.5906 - val_loss: 20.4408\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.5586 - val_loss: 20.2793\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.5088 - val_loss: 20.2346\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.4354 - val_loss: 20.1178\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.3114 - val_loss: 19.9323\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.1046 - val_loss: 19.5887\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.7396 - val_loss: 19.0290\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.2240 - val_loss: 18.2983\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.6453 - val_loss: 17.5826\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.1205 - val_loss: 17.0009\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.7364 - val_loss: 16.5774\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.4684 - val_loss: 16.3335\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.2813 - val_loss: 16.1840\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.1740 - val_loss: 16.0968\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.0668 - val_loss: 15.9708\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.9942 - val_loss: 15.8293\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.9276 - val_loss: 15.7078\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.8679 - val_loss: 15.6788\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.7919 - val_loss: 15.4904\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.7196 - val_loss: 15.3474\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6155 - val_loss: 15.1266\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4898 - val_loss: 14.8597\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3440 - val_loss: 14.6077\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.1850 - val_loss: 14.1345\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.0205 - val_loss: 13.8516\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.8614 - val_loss: 13.5512\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.7172 - val_loss: 13.2520\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.5794 - val_loss: 13.1808\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.4679 - val_loss: 12.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 23:58:26,160 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:58:26,160 - INFO - target_dir : ../Sample_data\n",
      "2023-08-25 23:58:26,163 - INFO - train_file num : 40\n",
      "2023-08-25 23:58:26,163 - INFO - train_file num : 40\n",
      "generate train_dataset: 100%|██████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 80.21it/s]\n",
      "2023-08-25 23:58:26,670 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_pump.pickle\n",
      "2023-08-25 23:58:26,670 - INFO - save_pickle -> ./pickle_pretrain_v1/pretrain_only_-6dB_pump.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== MODEL TRAINING ==============\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 320)]             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 320)               20800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50760 (198.28 KB)\n",
      "Trainable params: 50760 (198.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 9ms/step - loss: 572.5749 - val_loss: 180.2846\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 58.4335 - val_loss: 25.0531\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.6728 - val_loss: 19.5918\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.3438 - val_loss: 18.9415\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.0532 - val_loss: 18.7301\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.0162 - val_loss: 18.6930\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.0077 - val_loss: 18.6802\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9991 - val_loss: 18.6543\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9922 - val_loss: 18.6709\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9852 - val_loss: 18.7294\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9758 - val_loss: 18.6461\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9687 - val_loss: 18.4979\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9606 - val_loss: 18.5642\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9464 - val_loss: 18.6317\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9290 - val_loss: 18.4334\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9087 - val_loss: 18.3327\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.8764 - val_loss: 18.3360\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.8241 - val_loss: 18.0693\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.7489 - val_loss: 17.9179\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.6283 - val_loss: 17.7943\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.4455 - val_loss: 17.1167\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.1125 - val_loss: 16.1876\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.0726 - val_loss: 13.5215\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.0264 - val_loss: 12.3999\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.7414 - val_loss: 12.1473\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6859 - val_loss: 12.0103\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6599 - val_loss: 11.8574\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6504 - val_loss: 11.8872\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6403 - val_loss: 11.9208\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6182 - val_loss: 11.9376\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6174 - val_loss: 11.8438\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6019 - val_loss: 11.8711\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5923 - val_loss: 11.8441\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5849 - val_loss: 11.8709\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5717 - val_loss: 11.7578\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5618 - val_loss: 11.6265\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5559 - val_loss: 11.7740\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5416 - val_loss: 11.6966\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5329 - val_loss: 11.5827\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5191 - val_loss: 11.5419\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5000 - val_loss: 11.5622\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4903 - val_loss: 11.5740\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4645 - val_loss: 11.4506\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.4485 - val_loss: 11.4180\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4337 - val_loss: 11.4627\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4063 - val_loss: 11.4045\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3861 - val_loss: 11.3329\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.3600 - val_loss: 11.1834\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3339 - val_loss: 11.3717\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3151 - val_loss: 11.1510\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# import python-library\n",
    "########################################################################\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.core\n",
    "import librosa.feature\n",
    "import yaml\n",
    "import logging\n",
    "import keras\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# setup STD I/O\n",
    "########################################################################\n",
    "# 로깅을 설정하고 초기화하는 부분\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"make_pretrain_v1.log\")\n",
    "logger = logging.getLogger(' ')\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# file I/O\n",
    "########################################################################\n",
    "#파일 입출력 관련 함수 선언 부분\n",
    "def save_pickle(filename, save_data):  \n",
    "    logger.info(\"save_pickle -> {}\".format(filename))\n",
    "    with open(filename, 'wb') as sf:\n",
    "        pickle.dump(save_data, sf)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    logger.info(\"load_pickle <- {}\".format(filename))\n",
    "    with open(filename, 'rb') as lf:\n",
    "        load_data = pickle.load(lf)\n",
    "    return load_data\n",
    "\n",
    "\n",
    "def file_load(wav_name, mono=False):\n",
    "    try:\n",
    "        return librosa.load(wav_name, sr=None, mono=mono)\n",
    "    except:\n",
    "        logger.error(\"file_broken or not exists!! : {}\".format(wav_name))\n",
    "\n",
    "\n",
    "def demux_wav(wav_name, channel=0):\n",
    "    try:\n",
    "        multi_channel_data, sr = file_load(wav_name)\n",
    "        if multi_channel_data.ndim <= 1:\n",
    "            return sr, multi_channel_data\n",
    "        return sr, np.array(multi_channel_data)[channel, :]\n",
    "    except ValueError as msg:\n",
    "        logger.warning(f'{msg}')\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# feature extractor\n",
    "########################################################################\n",
    "#소리 파일 하나를 로그멜스펙트로그램 형태로 바꾸고 딥러닝 모델에 넣을 형태로 바꾸는 함수 \n",
    "def file_to_vector_array(file_name, n_mels=64, frames=5, n_fft=1024, hop_length=512, power=2.0):\n",
    "    dims = n_mels * frames\n",
    "    sr, y = demux_wav(file_name)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
    "    log_mel_spectrogram = 20.0 / power * np.log10(mel_spectrogram + sys.float_info.epsilon)\n",
    "    vectorarray_size = len(log_mel_spectrogram[0, :]) - frames + 1\n",
    "    if vectorarray_size < 1:\n",
    "        return np.empty((0, dims), float)\n",
    "    vectorarray = np.zeros((vectorarray_size, dims), float)\n",
    "    for t in range(frames):\n",
    "        vectorarray[:, n_mels * t: n_mels * (t + 1)] = log_mel_spectrogram[:, t: t + vectorarray_size].T \n",
    "    return vectorarray\n",
    "\n",
    "#소리 파일들의 이름명이 담긴 리스트를 입력하면 그것들을 하나의 데이터셋으로 합치는 함수\n",
    "def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, frames=5, n_fft=1024, hop_length=512, power=2.0):\n",
    "    dims = n_mels * frames\n",
    "    for idx in tqdm(range(len(file_list)), desc=msg):\n",
    "        vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, frames=frames, n_fft=n_fft, hop_length=hop_length,power=power)\n",
    "        if idx == 0:\n",
    "            dataset = np.zeros((vector_array.shape[0] * len(file_list), dims), float)\n",
    "        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array       \n",
    "    return dataset\n",
    "\n",
    "#기계 타입이 주어지면 id 전부의 normal 소리음들을 합쳐서 하나의 데이터셋으로 만드는 함수\n",
    "def dataset_generator(target_dir, machine_type, normal_dir_name=\"normal\", ext=\"wav\"):\n",
    "    logger.info(\"target_dir : {}\".format(target_dir))\n",
    "    train_files = []\n",
    "    machine_id = f\"-6dB_{machine_type}\"\n",
    "    machine = f\"{machine_type}\"\n",
    "    id_list = [\"id_00\", \"id_02\", \"id_04\", \"id_06\"]\n",
    "    machine_type_path = os.path.join(target_dir, machine_id, machine)\n",
    "    for id_ in id_list:\n",
    "        machine_id = f\"{id_}\"\n",
    "        machine_id_path = os.path.join(machine_type_path, machine_id, normal_dir_name)\n",
    "        normal_files = sorted(glob.glob(os.path.join(machine_id_path, f\"*.{ext}\")))\n",
    "        train_files.extend(normal_files)  \n",
    "    logger.info(\"train_file num : {num}\".format(num=len(train_files)))\n",
    "    return train_files\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# keras model\n",
    "########################################################################\n",
    "#오토인코더 함수\n",
    "def keras_model(inputDim):\n",
    "    inputLayer = Input(shape=(inputDim,))\n",
    "    h = Dense(64, activation=\"relu\")(inputLayer)    \n",
    "    h = Dense(64, activation=\"relu\")(h)\n",
    "    h = Dense(8, activation=\"relu\")(h)\n",
    "    h = Dense(64, activation=\"relu\")(h)\n",
    "    h = Dense(64, activation=\"relu\")(h)\n",
    "    h = Dense(inputDim, activation=None)(h)\n",
    "    return Model(inputs=inputLayer, outputs=h)\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# main\n",
    "########################################################################\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"make_pretrain_v1.yaml\", encoding = 'utf-8') as stream:\n",
    "        pretrain_v1_param = yaml.safe_load(stream)\n",
    "    \n",
    "    # pickle데이터, model데이터, result데이터가 저장될 파일과 폴더들 관련 변수 선언\n",
    "    os.makedirs(pretrain_v1_param[\"pickle_directory\"], exist_ok=True)\n",
    "    os.makedirs(pretrain_v1_param[\"model_directory\"], exist_ok=True)\n",
    "    pretrain_v1_data_dir = pretrain_v1_param[\"base_directory\"]\n",
    "\n",
    "    machine_types = [\"fan\", \"valve\", \"slider\", \"pump\"]\n",
    "    print(\"\\n===========================\")\n",
    "    \n",
    "    #각 기계 타입 별 normal 데이터 전체가 합쳐진 데이터를 생성하고 pretrain학습 시작 부분\n",
    "    for machine_type in machine_types:\n",
    "        db = f\"-6dB_{machine_type}\"\n",
    "        train_pickle = \"{pickle}/pretrain_only_{db}.pickle\".format(pickle=pretrain_v1_param[\"pickle_directory\"],db=db)\n",
    "        \n",
    "        #데이터셋 생성, pickle에 저장돼있다면 그것을 사용\n",
    "        if os.path.exists(train_pickle):\n",
    "            train_data = load_pickle(train_pickle)\n",
    "        else:\n",
    "            train_files = dataset_generator(pretrain_v1_data_dir, machine_type)\n",
    "            train_data = list_to_vector_array(train_files,\n",
    "                                              msg=\"generate train_dataset\",\n",
    "                                              n_mels=pretrain_v1_param[\"feature\"][\"n_mels\"],\n",
    "                                              frames=pretrain_v1_param[\"feature\"][\"frames\"],\n",
    "                                              n_fft=pretrain_v1_param[\"feature\"][\"n_fft\"],\n",
    "                                              hop_length=pretrain_v1_param[\"feature\"][\"hop_length\"],\n",
    "                                              power=pretrain_v1_param[\"feature\"][\"power\"])\n",
    "            save_pickle(train_pickle, train_data)\n",
    "\n",
    "        print(\"============== MODEL TRAINING ==============\")\n",
    "        model_directory = pretrain_v1_param[\"model_directory\"]\n",
    "        model_file = \"{model}/pretrain_only_{db}.h5\".format(model=model_directory, db = db)\n",
    "        if not os.path.exists(model_file):                                                                           \n",
    "            #모델 생성, 학습, 및 저장\n",
    "            model = keras_model(pretrain_v1_param[\"feature\"][\"n_mels\"] * pretrain_v1_param[\"feature\"][\"frames\"])\n",
    "            model.summary()                    \n",
    "            model.compile(**pretrain_v1_param[\"fit\"][\"compile\"])\n",
    "            model.fit(train_data,                 \n",
    "                      train_data,\n",
    "                      epochs=pretrain_v1_param[\"fit\"][\"epochs\"],\n",
    "                      batch_size=pretrain_v1_param[\"fit\"][\"batch_size\"],\n",
    "                      shuffle=pretrain_v1_param[\"fit\"][\"shuffle\"],\n",
    "                      validation_split=pretrain_v1_param[\"fit\"][\"validation_split\"],\n",
    "                      verbose=pretrain_v1_param[\"fit\"][\"verbose\"])\n",
    "            model.save(model_file)        \n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40cc64-d957-4f94-8bbb-0e8782580bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f0342-6979-4fce-a2b1-3eeac4ddde33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "557b26f2afb0034d2b9583f22e5a96d5c2fd0b947b12212a0c5ba344d367ad03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
